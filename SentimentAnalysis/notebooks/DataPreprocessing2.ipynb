{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106747e5",
   "metadata": {},
   "source": [
    "# Data Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4eece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05268832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#TF 2.3 \n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea225db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#TF 2.3 \n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e213b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.getcwd()+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "337f2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl\n",
    "#!pip install matplotlib\n",
    "#!pip install transformers\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a1bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907c2c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The website was very easy to use and my insura...</td>\n",
       "      <td>website</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The web sight was easy to understand and I got...</td>\n",
       "      <td>web sight</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Having filled in the application on-line I cou...</td>\n",
       "      <td>point</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After finding AXA was cheaper than my renewal ...</td>\n",
       "      <td>prices</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The quote was a reasonable price compared with...</td>\n",
       "      <td>insurances</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence      Entity Sentiment\n",
       "0  The website was very easy to use and my insura...     website  positive\n",
       "1  The web sight was easy to understand and I got...   web sight  positive\n",
       "2  Having filled in the application on-line I cou...       point  negative\n",
       "3  After finding AXA was cheaper than my renewal ...      prices  positive\n",
       "4  The quote was a reasonable price compared with...  insurances  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('data\\train.tsv', sep='\\t')\n",
    "df = pd.read_excel(\"data\\Entity_sentiment_trainV2.xlsx\")#pd.read_excel('data\\Entity_sentiment_trainV2.xlsx', sheetname=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6332552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFC', 'Quite refreshing from my other insurance company and also a Â£300 save.') == 'Quite refreshing from my other insurance company and also a A£300 save.'\n",
    "#unicodedata.normalize('NFKD', 'Quite refreshing from my other insurance company and also a Â£300 save.') == 'Quite refreshing from my other insurance company and also a A£300 save.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5affaf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks for reading'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unicodedata.normalize('NFKD', 'Quite refreshing from my other insurance company and also a Â£300 save.')\n",
    "#Ⓣⓗⓐⓝⓚⓢ ⓕⓞⓡ ⓡⓔⓐⓓⓘⓝⓖ\n",
    "unicodedata.normalize('NFKD', 'Ⓣⓗⓐⓝⓚⓢ ⓕⓞⓡ ⓡⓔⓐⓓⓘⓝⓖ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dfaf1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The website was very easy to use and my insura...</td>\n",
       "      <td>website</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The web sight was easy to understand and I got...</td>\n",
       "      <td>web sight</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Having filled in the application on-line I cou...</td>\n",
       "      <td>point</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After finding AXA was cheaper than my renewal ...</td>\n",
       "      <td>prices</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The quote was a reasonable price compared with...</td>\n",
       "      <td>insurances</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence      Entity  Sentiment\n",
       "0  The website was very easy to use and my insura...     website          1\n",
       "1  The web sight was easy to understand and I got...   web sight          1\n",
       "2  Having filled in the application on-line I cou...       point          0\n",
       "3  After finding AXA was cheaper than my renewal ...      prices          1\n",
       "4  The quote was a reasonable price compared with...  insurances          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_mapping = {'negative': 0,\n",
    " 'positive': 1}\n",
    "\n",
    "\n",
    "df['Sentiment'] = df['Sentiment'].map(binary_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66ae1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5999, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "seq_len = 512\n",
    "num_samples = len(df)\n",
    "\n",
    "num_samples, seq_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac3c1420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# initialize tokenizer\\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\\n\\n# tokenize - this time returning Numpy tensors\\ntokens = tokenizer(df['Sentence'].tolist(), max_length=seq_len, truncation=True,\\n                   padding='max_length', add_special_tokens=True,\\n                   return_tensors='np')\\n                   \\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "n=5\n",
    "seed = 4568\n",
    "kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True) \n",
    "\n",
    "'''\n",
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# tokenize - this time returning Numpy tensors\n",
    "tokens = tokenizer(df['Sentence'].tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np')\n",
    "                   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7215047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57edab00",
   "metadata": {},
   "source": [
    "def map_func(input_ids, masks, entities, labels):\n",
    "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "    return {'input_ids': input_ids, 'entities':entities, 'attention_mask': masks}, labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88954fcb",
   "metadata": {},
   "source": [
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "# we can view the model using the summary method\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "526189ee",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "# convert bert embeddings into 2 output classes\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "691463a0",
   "metadata": {},
   "source": [
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# (optional) freeze bert layer\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# print out model summary\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3be5f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate result: 1.0\n",
      "Final result: 3.0\n"
     ]
    }
   ],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.cast(y_pred, tf.bool)\n",
    "\n",
    "        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "        values = tf.cast(values, self.dtype)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "\n",
    "m = F1Score()\n",
    "m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n",
    "print('Intermediate result:', float(m.result()))\n",
    "\n",
    "m.update_state([1, 1, 1, 1], [0, 1, 1, 0])\n",
    "print('Final result:', float(m.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d6a9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(precision, recall):\n",
    "    f1 = ((precision*recall)/(precision+recall))*2\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e167f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return '\\model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9031572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e67b3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "def create_Model():\n",
    "    \n",
    "    bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "    #bert = TFAutoModel.from_pretrained('princeton-nlp/sup-simcse-bert-base-uncased')#princeton-nlp/unsup-simcse-bert-base-uncased')\n",
    "    #bert = TFAutoModel.from_pretrained('princeton-nlp/unsup-simcse-bert-base-uncased')\n",
    "    # we can view the model using the summary method\n",
    "    bert.summary()\n",
    "    \n",
    "    # two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "    #entities = tf.keras.layers.Input(shape=(512,), name='entities', dtype='int32')\n",
    "\n",
    "    \n",
    "    # we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "    embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "    #embeddings = bert.bert(input_ids, entities, attention_mask=mask )\n",
    "    # convert bert embeddings into 2 output classes\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "    y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "    # (optional) freeze bert layer\n",
    "    model.layers[2].trainable = False\n",
    "\n",
    "    # print out model summary\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "    f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\n",
    "    #f1  = tf.keras.metrics.CategoricalAccuracy('f1') num_classes=1, threshold=0.5\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    #model.add\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[acc, recall, precision])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "249f1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16, 2)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float64)>\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16, 2)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float64)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         787456      bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 2)            2050        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,099,778\n",
      "Trainable params: 789,506\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n",
      "\\model_1.h5\n",
      "C:\\NLP\\SentimentAnalysis\\saved_models\\model_1.h5\n",
      "Epoch 1/3\n",
      "299/299 [==============================] - 178s 582ms/step - loss: 0.6103 - accuracy: 0.6775 - recall: 0.6775 - precision: 0.6775 - val_loss: 0.5885 - val_accuracy: 0.6892 - val_recall: 0.6892 - val_precision: 0.6892\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.68917, saving model to C:\\NLP\\SentimentAnalysis\\saved_models\\model_1.h5\n",
      "Epoch 2/3\n",
      "299/299 [==============================] - 172s 576ms/step - loss: 0.5787 - accuracy: 0.6869 - recall: 0.6869 - precision: 0.6869 - val_loss: 0.5661 - val_accuracy: 0.7100 - val_recall: 0.7100 - val_precision: 0.7100\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.68917 to 0.71000, saving model to C:\\NLP\\SentimentAnalysis\\saved_models\\model_1.h5\n",
      "Epoch 3/3\n",
      " 16/299 [>.............................] - ETA: 2:13 - loss: 0.5238 - accuracy: 0.7617 - recall: 0.7617 - precision: 0.7617"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9084/1896314330.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;31m# Model weights are saved at the end of every epoch, if it's the best seen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;31m# so far.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     model.fit(train_dataset, \n\u001b[0m\u001b[0;32m    126\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\envLogically\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\envLogically\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\envLogically\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\envLogically\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\envLogically\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\envLogically\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\envLogically\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "#import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "logdir = str.format('logs\\scalars\\{0}', datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('princeton-nlp/sup-simcse-bert-base-uncased')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-bert-base-uncased')\n",
    "#tokenizer.encode(add_special_tokens=True, text=)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True) \n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDAITON_LOSS = []\n",
    "batch_size = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "save_dir = os.getcwd()+'\\saved_models'\n",
    "fold_var = 1\n",
    "Y = df[['Sentiment']]\n",
    "#X = df['Sentence']\n",
    "X = df['Sentence'] +\" : \"+ df['Entity']\n",
    "for train_index, val_index in skf.split(X,Y):\n",
    "    training_data = X.iloc[train_index]\n",
    "    validation_data = X.iloc[val_index]\n",
    "    #print(train_index + '| '+ training_data + ' | |'+ val_index + '|' +validation_data)\n",
    "    #print(str.format('{0} | {1} || {2} | {3}', train_index, training_data, val_index, validation_data))\n",
    "    \n",
    "    #-------------------------------------------------Train DataSet ----------------------------------------------------#\n",
    "    num_samples = len(training_data)\n",
    "    ###arr = validation_data.values\n",
    "    \n",
    "    train_lbl =  Y.iloc[train_index]\n",
    "    train_arr = np.array(train_lbl['Sentiment'].values.tolist())\n",
    "\n",
    "    train_labels = np.zeros((num_samples, train_arr.max()+1))\n",
    "\n",
    "    \n",
    "    train_labels[np.arange(num_samples), train_arr] = 1\n",
    "\n",
    "\n",
    "    # tokenize - this time returning Numpy tensors\n",
    "    \n",
    "    train_tokens = tokenizer(training_data.tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np') \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_tokens['input_ids'], \n",
    "                                                        train_tokens['attention_mask'], \n",
    "                                                        train_labels))\n",
    "    train_dataset = train_dataset.map(map_func)\n",
    "    #dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "  \n",
    "\n",
    "    print(train_dataset.take(1))\n",
    "    #-------------------------------------------------Validation DataSet ----------------------------------------------------#\n",
    "    num_samples = len(validation_data)\n",
    "    ###arr = validation_data.values\n",
    "    \n",
    "    val_lbl =  Y.iloc[val_index]\n",
    "    val_arr = np.array(val_lbl['Sentiment'].values.tolist())\n",
    "    ###arr = lbl['Sentiment']\n",
    "    #print(type(arr))\n",
    "    \n",
    "    #print(num_samples)\n",
    "    #print(arr)\n",
    "    val_labels = np.zeros((num_samples, val_arr.max()+1))\n",
    "    #print(labels.shape)\n",
    "    \n",
    "    val_labels[np.arange(num_samples), val_arr] = 1\n",
    "\n",
    "    #print(labels)\n",
    "\n",
    "    # tokenize - this time returning Numpy tensors\n",
    "    val_tokens = tokenizer(validation_data.tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np')\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((val_tokens['input_ids'], \n",
    "                                                             val_tokens['attention_mask'], \n",
    "                                                             val_labels))\n",
    "    validation_dataset = validation_dataset.map(map_func)\n",
    "    validation_dataset = validation_dataset.batch(batch_size, drop_remainder=True)\n",
    "    #batch_size = 16\n",
    "\n",
    "    print(validation_dataset.take(1))\n",
    "\n",
    "    model = create_Model()\n",
    "\n",
    "    \n",
    "    # CREATE CALLBACKS\n",
    "    '''\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var), \n",
    "                            monitor='val_accuracy', verbose=1, \n",
    "                            save_best_only=True, mode='max')\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    \n",
    "    callbacks_list = [checkpoint, tensorboard_callback]\n",
    "    \n",
    "    history = model.fit(train_dataset, validation_data=validation_dataset, epochs=3, callbacks=callbacks_list, verbose=1)\n",
    "    \n",
    "    model.load_weights(save_dir +\"\\model_\"+str(fold_var)+\".h5\")\n",
    "    '''\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    checkpoint_filepath = save_dir + get_model_name(fold_var)\n",
    "    #model.load_weights(checkpoint_filepath)\n",
    "    print(get_model_name(fold_var))\n",
    "    print(checkpoint_filepath)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                                   save_weights_only=False,\n",
    "                                                                   monitor='val_accuracy',\n",
    "                                                                   mode='max',\n",
    "                                                                   verbose=1,\n",
    "                                                                   save_best_only=True)\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen\n",
    "    # so far.\n",
    "    model.fit(train_dataset, \n",
    "              validation_data=validation_dataset,\n",
    "              epochs=EPOCHS, \n",
    "              callbacks=[model_checkpoint_callback, tensorboard_callback, earlyStopping], \n",
    "              verbose=1)\n",
    "\n",
    "    # The model weights (that are considered the best) are loaded into the model.\n",
    "    model.load_weights(save_dir +\"\\model_\"+str(fold_var)+\".h5\")\n",
    "    #----------------------------------------------------------------------------\n",
    "    #results = model.evaluate(validation_data)\n",
    "    #results = dict(zip(model.metrics_names,results))\n",
    "    \n",
    "    #VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    #VALIDATION_LOSS.append(results['loss'])\n",
    "    \n",
    "    fold_var +=1\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d478d0b3",
   "metadata": {},
   "source": [
    "<TakeDataset shapes: ({input_ids: (512,), attention_mask: (512,)}, (2,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float64)>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa0cd6a2",
   "metadata": {},
   "source": [
    "path = '\\data'\n",
    "with open('review-xids.npy', 'wb') as f:\n",
    "    np.save(f, tokens['input_ids'])\n",
    "with open('review-xmask.npy', 'wb') as f:\n",
    "    np.save(f, tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4052b996",
   "metadata": {},
   "source": [
    "del tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2b63d70",
   "metadata": {},
   "source": [
    "# extract Sentiment column\n",
    "arr = df['Sentiment'].values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd41a22e",
   "metadata": {},
   "source": [
    "labels = np.zeros((num_samples, arr.max()+1))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64de7a38",
   "metadata": {},
   "source": [
    "labels[np.arange(num_samples), arr] = 1\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540b6de",
   "metadata": {},
   "source": [
    "with open('review-labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df93be5",
   "metadata": {},
   "source": [
    "# Input Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b46ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3834b8c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('review-xids.npy', 'rb') as f:\n",
    "    Xids = np.load(f, allow_pickle=True)\n",
    "with open('review-xmask.npy', 'rb') as f:\n",
    "    Xmask = np.load(f, allow_pickle=True)\n",
    "with open('review-labels.npy', 'rb') as f:\n",
    "    labels = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bd18901",
   "metadata": {},
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a12a1657",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "898a4451",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))[10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07d93999",
   "metadata": {},
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "# then we use the dataset map method to apply this transformation\n",
    "dataset = dataset.map(map_func)\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f093cb8",
   "metadata": {},
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaa1574c",
   "metadata": {},
   "source": [
    "tf.data.experimental.save(dataset, 'train_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b8cdd",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.randn(15, 3)\n",
    "y = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "for train_index, test_index in k_fold.split(dataset, y):\n",
    "    print(f\"Train index: {train_index} Test index: {test_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e3ccc",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "#x = np.random.randn(15, 3)\n",
    "#y = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "for train_index, test_index in k_fold.split(dataset, y):\n",
    "    print(f\"Train index: {train_index} Test index: {test_index}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88e9e134",
   "metadata": {},
   "source": [
    "split = 0.9\n",
    "\n",
    "# we need to calculate how many batches must be taken to create 90% training set\n",
    "size = int((Xids.shape[0] / batch_size) * split)\n",
    "\n",
    "size"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb6b6525",
   "metadata": {},
   "source": [
    "train_ds = dataset.take(size)\n",
    "val_ds = dataset.skip(size)\n",
    "\n",
    "# free up memory\n",
    "#del dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80ba8ebe",
   "metadata": {},
   "source": [
    "tf.data.experimental.save(train_ds, 'train')\n",
    "tf.data.experimental.save(val_ds, 'val')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5e279b4",
   "metadata": {},
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5588cf2",
   "metadata": {},
   "source": [
    "val_ds.element_spec == train_ds.element_spec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77b2e056",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "n=5\n",
    "seed = 4568\n",
    "kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True) \n",
    "\n",
    "save_dir = '/saved_models/'\n",
    "num_epochs = 3\n",
    "results = []\n",
    "#train_data = tf.data.experimental.load('train', element_spec=element_spec)\n",
    "train_X = df['Sentence']\n",
    "train_Y =  df['Sentiment']\n",
    "#for train_index, val_index in kf.split(train_data):\n",
    "fold_var = 1\n",
    "dataset\n",
    "for train_index, val_index in skf.split(dataset,labels.shape[0]):\n",
    "  # splitting Dataframe (dataset not included)\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "    #train_df = train_X.iloc[train_index]\n",
    "    #val_df = train_X.iloc[val_index]\n",
    "    # Defining Model\n",
    "    print(train_index)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b0fe192",
   "metadata": {},
   "source": [
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0da8f79",
   "metadata": {},
   "source": [
    "ds = tf.data.experimental.load('train', element_spec=dataset.element_spec)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7e99e50",
   "metadata": {},
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "# we can view the model using the summary method\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f372cf9",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "# convert bert embeddings into 2 output classes\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b552ff6",
   "metadata": {},
   "source": [
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# (optional) freeze bert layer\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# print out model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5476c10c",
   "metadata": {},
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f0ee57b",
   "metadata": {},
   "source": [
    "element_spec = ({'input_ids': tf.TensorSpec(shape=(16, 512), dtype=tf.int32, name=None),\n",
    "                 'attention_mask': tf.TensorSpec(shape=(16, 512), dtype=tf.int32, name=None)},\n",
    "                tf.TensorSpec(shape=(16, 2), dtype=tf.float64, name=None))\n",
    "\n",
    "# load the training and validation sets\n",
    "#train_ds = tf.data.experimental.load('train', element_spec=element_spec)\n",
    "#val_ds = tf.data.experimental.load('val', element_spec=element_spec)\n",
    "\n",
    "# view the input format\n",
    "#train_ds.take(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8340a54d",
   "metadata": {},
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f7549bf",
   "metadata": {},
   "source": [
    "skf = StratifiedKFold(n_split = 5, random_state = 7, shuffle = True) \n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDAITON_LOSS = []\n",
    "\n",
    "\n",
    "save_dir = '/saved_models/'\n",
    "fold_var = 1\n",
    "for train_index, val_index in skf.split(np.zeros(n),Y):\n",
    "    training_data = train_data.iloc[train_index]\n",
    "    validation_data = train_data.iloc[val_index]\n",
    "    train_data_generator = idg.flow_from_dataframe(training_data, directory = image_dir,\n",
    "                            x_col = \"filename\", y_col = \"label\",\n",
    "                            class_mode = \"categorical\", shuffle = True)\n",
    "    valid_data_generator  = idg.flow_from_dataframe(validation_data, directory = image_dir,\n",
    "                            x_col = \"filename\", y_col = \"label\",\n",
    "                            class_mode = \"categorical\", shuffle = True)\n",
    "    \n",
    "    \n",
    "    # CREATE NEW MODEL\n",
    "    #model = create_new_model()\n",
    "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "    # (optional) freeze bert layer\n",
    "    model.layers[2].trainable = False\n",
    "\n",
    "    # print out model summary\n",
    "    #model.summary()\n",
    "    \n",
    "    # COMPILE NEW MODEL\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "    # CREATE CALLBACKS\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var), \n",
    "                            monitor='val_accuracy', verbose=1, \n",
    "                            save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # There can be other callbacks, but just showing one because it involves the model name\n",
    "    # This saves the best model\n",
    "    # FIT THE MODEL\n",
    "    ''' history = model.fit(train_data_generator,\n",
    "                epochs=num_epochs,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_data=valid_data_generator)\n",
    "    '''\n",
    "    history = model.fit(\n",
    "                        train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        callbacks=callbacks_list,\n",
    "                        epochs=num_epochs\n",
    "                        )\n",
    "    \n",
    "    #PLOT HISTORY\n",
    "    #       :\n",
    "    #       :\n",
    "    \n",
    "    # LOAD BEST MODEL to evaluate the performance of the model\n",
    "    model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n",
    "    \n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names,results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e373de8",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "model.save('sentiment_model')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5163d9e9",
   "metadata": {},
   "source": [
    "n=5\n",
    "seed = \n",
    "kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for train_index, val_index in kf.split(train_data):\n",
    "  # splitting Dataframe (dataset not included)\n",
    "    train_df = train_data.iloc[train_index]\n",
    "    val_df = train_data.iloc[val_index]\n",
    "    # Defining Model\n",
    "    model = ClassificationModel('bert', 'bert-base-uncased') \n",
    "  # train the model\n",
    "    model.train_model(train_df)\n",
    "  # validate the model \n",
    "    result, model_outputs, wrong_predictions = model.eval_model(val_df, acc=accuracy_score)\n",
    "    print(result['acc'])\n",
    "  # append model score\n",
    "    results.append(result['acc'])\n",
    "\n",
    "\n",
    "print(\"results\",results)\n",
    "print(f\"Mean-Precision: {sum(results) / len(results)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e391979a",
   "metadata": {},
   "source": [
    "#train_data\n",
    "#df[['Sentence', 'Sentiment']]\n",
    "df['Sentence']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4e9428b",
   "metadata": {},
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c29ca1e",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "n=5\n",
    "seed = 4568\n",
    "kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True) \n",
    "\n",
    "save_dir = '/saved_models/'\n",
    "num_epochs = 3\n",
    "results = []\n",
    "#train_data = tf.data.experimental.load('train', element_spec=element_spec)\n",
    "train_X = df['Sentence']\n",
    "train_Y =  df['Sentiment']\n",
    "#for train_index, val_index in kf.split(train_data):\n",
    "fold_var = 1\n",
    "for train_index, val_index in skf.split(train_X,train_Y):\n",
    "  # splitting Dataframe (dataset not included)\n",
    "    train_df = train_X.iloc[train_index]\n",
    "    val_df = train_X.iloc[val_index]\n",
    "    # Defining Model\n",
    "    \n",
    "    bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "    # we can view the model using the summary method\n",
    "    bert.summary()\n",
    "    # two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    # we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "    embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "    # convert bert embeddings into 2 output classes\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "    y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "    # (optional) freeze bert layer\n",
    "    model.layers[2].trainable = False\n",
    "\n",
    "    # print out model summary\n",
    "    model.summary()\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "    \n",
    "    # CREATE CALLBACKS\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var), \n",
    "                            monitor='val_accuracy', verbose=1, \n",
    "                            save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    \n",
    "    element_spec = ({'input_ids': tf.TensorSpec(shape=(16, 512), dtype=tf.int32, name=None),\n",
    "                 'attention_mask': tf.TensorSpec(shape=(16, 512), dtype=tf.int32, name=None)},\n",
    "                tf.TensorSpec(shape=(16, 2), dtype=tf.float64, name=None))\n",
    "#-------------------------------------------\n",
    "    \n",
    "    #model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "    # train the model\n",
    "    #model.train_model(train_df)\n",
    "    history = model.fit(\n",
    "                        train_df,\n",
    "                        validation_data=val_df,\n",
    "                        callbacks=callbacks_list,\n",
    "                        epochs=num_epochs\n",
    "                        )\n",
    "    # validate the model \n",
    "    #result, model_outputs, wrong_predictions = model.eval_model(val_df, acc=accuracy_score)\n",
    "    #print(result['acc'])\n",
    "    # append model score\n",
    "    #results.append(result['acc'])\n",
    "    model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n",
    "    \n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names,results))\n",
    "    \n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1\n",
    "\n",
    "\n",
    "print(\"results\",results)\n",
    "print(f\"Mean-Precision: {sum(results) / len(results)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26549bfc",
   "metadata": {},
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Dataset\n",
    "dataset = [[\"Example sentence belonging to class 1\", 1],\n",
    "      [\"Example sentence belonging to class 0\", 0],\n",
    "      [\"Example eval sentence belonging to class 1\", 1],\n",
    "      [\"Example eval sentence belonging to class 0\", 0]]\n",
    "train_data = pd.DataFrame(dataset)\n",
    "\n",
    "# prepare cross validation\n",
    "n=5\n",
    "kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for train_index, val_index in kf.split(train_data):\n",
    "  # splitting Dataframe (dataset not included)\n",
    "    train_df = train_data.iloc[train_index]\n",
    "    val_df = train_data.iloc[val_index]\n",
    "    # Defining Model\n",
    "    model = ClassificationModel('bert', 'bert-base-uncased') \n",
    "  # train the model\n",
    "    model.train_model(train_df)\n",
    "  # validate the model \n",
    "    result, model_outputs, wrong_predictions = model.eval_model(val_df, acc=accuracy_score)\n",
    "    print(result['acc'])\n",
    "  # append model score\n",
    "    results.append(result['acc'])\n",
    "\n",
    "\n",
    "print(\"results\",results)\n",
    "print(f\"Mean-Precision: {sum(results) / len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d328a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
