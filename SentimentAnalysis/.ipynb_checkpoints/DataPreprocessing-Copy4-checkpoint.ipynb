{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106747e5",
   "metadata": {},
   "source": [
    "# Data Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4eece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05268832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#TF 2.3 \n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea225db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#TF 2.3 \n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e213b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#print(os.getcwd()+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "337f2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl\n",
    "#!pip install matplotlib\n",
    "#!pip install transformers\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a1bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907c2c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The website was very easy to use and my insura...</td>\n",
       "      <td>website</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The web sight was easy to understand and I got...</td>\n",
       "      <td>web sight</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Having filled in the application on-line I cou...</td>\n",
       "      <td>point</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After finding AXA was cheaper than my renewal ...</td>\n",
       "      <td>prices</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The quote was a reasonable price compared with...</td>\n",
       "      <td>insurances</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence      Entity Sentiment\n",
       "0  The website was very easy to use and my insura...     website  positive\n",
       "1  The web sight was easy to understand and I got...   web sight  positive\n",
       "2  Having filled in the application on-line I cou...       point  negative\n",
       "3  After finding AXA was cheaper than my renewal ...      prices  positive\n",
       "4  The quote was a reasonable price compared with...  insurances  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('data\\train.tsv', sep='\\t')\n",
    "df = pd.read_excel(\"data\\Entity_sentiment_trainV2.xlsx\")#pd.read_excel('data\\Entity_sentiment_trainV2.xlsx', sheetname=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6332552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFC', 'Quite refreshing from my other insurance company and also a Â£300 save.') == 'Quite refreshing from my other insurance company and also a A£300 save.'\n",
    "#unicodedata.normalize('NFKD', 'Quite refreshing from my other insurance company and also a Â£300 save.') == 'Quite refreshing from my other insurance company and also a A£300 save.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5affaf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks for reading'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unicodedata.normalize('NFKD', 'Quite refreshing from my other insurance company and also a Â£300 save.')\n",
    "#Ⓣⓗⓐⓝⓚⓢ ⓕⓞⓡ ⓡⓔⓐⓓⓘⓝⓖ\n",
    "unicodedata.normalize('NFKD', 'Ⓣⓗⓐⓝⓚⓢ ⓕⓞⓡ ⓡⓔⓐⓓⓘⓝⓖ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dfaf1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The website was very easy to use and my insura...</td>\n",
       "      <td>website</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The web sight was easy to understand and I got...</td>\n",
       "      <td>web sight</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Having filled in the application on-line I cou...</td>\n",
       "      <td>point</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After finding AXA was cheaper than my renewal ...</td>\n",
       "      <td>prices</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The quote was a reasonable price compared with...</td>\n",
       "      <td>insurances</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence      Entity  Sentiment\n",
       "0  The website was very easy to use and my insura...     website          1\n",
       "1  The web sight was easy to understand and I got...   web sight          1\n",
       "2  Having filled in the application on-line I cou...       point          0\n",
       "3  After finding AXA was cheaper than my renewal ...      prices          1\n",
       "4  The quote was a reasonable price compared with...  insurances          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_mapping = {'negative': 0,\n",
    " 'positive': 1}\n",
    "\n",
    "\n",
    "df['Sentiment'] = df['Sentiment'].map(binary_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66ae1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5999, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "seq_len = 512\n",
    "num_samples = len(df)\n",
    "\n",
    "num_samples, seq_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac3c1420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# initialize tokenizer\\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\\n\\n# tokenize - this time returning Numpy tensors\\ntokens = tokenizer(df['Sentence'].tolist(), max_length=seq_len, truncation=True,\\n                   padding='max_length', add_special_tokens=True,\\n                   return_tensors='np')\\n                   \\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "n=5\n",
    "seed = 4568\n",
    "kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True) \n",
    "\n",
    "'''\n",
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# tokenize - this time returning Numpy tensors\n",
    "tokens = tokenizer(df['Sentence'].tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np')\n",
    "                   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7215047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57edab00",
   "metadata": {},
   "source": [
    "def map_func(input_ids, masks, entities, labels):\n",
    "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "    return {'input_ids': input_ids, 'entities':entities, 'attention_mask': masks}, labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88954fcb",
   "metadata": {},
   "source": [
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "# we can view the model using the summary method\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "526189ee",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "# convert bert embeddings into 2 output classes\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "691463a0",
   "metadata": {},
   "source": [
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# (optional) freeze bert layer\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# print out model summary\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3be5f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate result: 1.0\n",
      "Final result: 3.0\n"
     ]
    }
   ],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.cast(y_pred, tf.bool)\n",
    "\n",
    "        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "        values = tf.cast(values, self.dtype)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "\n",
    "m = F1Score()\n",
    "m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n",
    "print('Intermediate result:', float(m.result()))\n",
    "\n",
    "m.update_state([1, 1, 1, 1], [0, 1, 1, 0])\n",
    "print('Final result:', float(m.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d6a9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(precision, recall):\n",
    "    f1 = ((precision*recall)/(precision+recall))*2\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e167f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return '\\model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9031572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e67b3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "def create_Model():\n",
    "    \n",
    "    bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "    #bert = TFAutoModel.from_pretrained('princeton-nlp/sup-simcse-bert-base-uncased')#princeton-nlp/unsup-simcse-bert-base-uncased')\n",
    "    #bert = TFAutoModel.from_pretrained('princeton-nlp/unsup-simcse-bert-base-uncased')\n",
    "    # we can view the model using the summary method\n",
    "    bert.summary()\n",
    "    \n",
    "    # two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "    #entities = tf.keras.layers.Input(shape=(512,), name='entities', dtype='int32')\n",
    "\n",
    "    \n",
    "    # we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "    embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "    #embeddings = bert.bert(input_ids, entities, attention_mask=mask )\n",
    "    # convert bert embeddings into 2 output classes\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(x)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "    # (optional) freeze bert layer\n",
    "    model.layers[2].trainable = False\n",
    "\n",
    "    # print out model summary\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "    f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.8)\n",
    "    #f1  = tf.keras.metrics.CategoricalAccuracy('f1') num_classes=1, threshold=0.5\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    #model.add\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[acc, recall, precision])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "249f1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n",
      "[1 0 1 ... 1 1 0]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n",
      "[1 1 1 ... 1 0 1]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         787456      bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 1)            1025        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,098,753\n",
      "Trainable params: 788,481\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n",
      "\\model_1.h5\n",
      "C:\\NLP\\SentimentAnalysis\\saved_models\\model_1.h5\n",
      "Epoch 1/3\n",
      "299/299 [==============================] - 168s 547ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall_3: 0.0000e+00 - precision_3: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall_3: 0.0000e+00 - val_precision_3: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 1.00000, saving model to C:\\NLP\\SentimentAnalysis\\saved_models\\model_1.h5\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `precision` which is not available. Available metrics are: loss,accuracy,recall_3,precision_3,val_loss,val_accuracy,val_recall_3,val_precision_3\n",
      "Epoch 2/3\n",
      "299/299 [==============================] - 164s 548ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall_3: 0.0000e+00 - precision_3: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall_3: 0.0000e+00 - val_precision_3: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 1.00000\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `precision` which is not available. Available metrics are: loss,accuracy,recall_3,precision_3,val_loss,val_accuracy,val_recall_3,val_precision_3\n",
      "Epoch 3/3\n",
      "299/299 [==============================] - 165s 551ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall_3: 0.0000e+00 - precision_3: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall_3: 0.0000e+00 - val_precision_3: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 1.00000\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `precision` which is not available. Available metrics are: loss,accuracy,recall_3,precision_3,val_loss,val_accuracy,val_recall_3,val_precision_3\n",
      "[1 0 1 ... 0 1 0]\n",
      "[1 0 1 ... 0 1 0]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n",
      "[1 1 1 ... 1 1 1]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         787456      bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 1)            1025        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,098,753\n",
      "Trainable params: 788,481\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n",
      "\\model_2.h5\n",
      "C:\\NLP\\SentimentAnalysis\\saved_models\\model_2.h5\n",
      "Epoch 1/3\n",
      "299/299 [==============================] - 169s 552ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 1.00000, saving model to C:\\NLP\\SentimentAnalysis\\saved_models\\model_2.h5\n",
      "Epoch 2/3\n",
      "299/299 [==============================] - 165s 553ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 1.00000\n",
      "Epoch 3/3\n",
      "299/299 [==============================] - 163s 545ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 1.00000\n",
      "[1 1 0 ... 1 1 0]\n",
      "[1 1 0 ... 1 1 0]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n",
      "[1 0 1 ... 1 1 0]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         787456      bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 1)            1025        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,098,753\n",
      "Trainable params: 788,481\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n",
      "\\model_3.h5\n",
      "C:\\NLP\\SentimentAnalysis\\saved_models\\model_3.h5\n",
      "Epoch 1/3\n",
      "299/299 [==============================] - 169s 550ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 1.00000, saving model to C:\\NLP\\SentimentAnalysis\\saved_models\\model_3.h5\n",
      "Epoch 2/3\n",
      "299/299 [==============================] - 163s 544ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 1.00000\n",
      "Epoch 3/3\n",
      "299/299 [==============================] - 162s 542ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 1.00000\n",
      "[1 1 1 ... 0 1 1]\n",
      "[1 1 1 ... 0 1 1]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n",
      "[0 1 1 ... 1 1 0]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         787456      bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 1)            1025        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,098,753\n",
      "Trainable params: 788,481\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n",
      "\\model_4.h5\n",
      "C:\\NLP\\SentimentAnalysis\\saved_models\\model_4.h5\n",
      "Epoch 1/3\n",
      "299/299 [==============================] - 166s 543ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0239 - precision: 0.8041 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 1.00000, saving model to C:\\NLP\\SentimentAnalysis\\saved_models\\model_4.h5\n",
      "Epoch 2/3\n",
      "299/299 [==============================] - 161s 538ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 1.00000\n",
      "Epoch 3/3\n",
      "299/299 [==============================] - 161s 538ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 1.00000\n",
      "[1 1 0 ... 1 1 0]\n",
      "[1 1 0 ... 1 1 0]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n",
      "[0 1 1 ... 0 1 1]\n",
      "<TakeDataset shapes: ({input_ids: (16, 512), attention_mask: (16, 512)}, (16,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         787456      bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 1)            1025        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,098,753\n",
      "Trainable params: 788,481\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n",
      "\\model_5.h5\n",
      "C:\\NLP\\SentimentAnalysis\\saved_models\\model_5.h5\n",
      "Epoch 1/3\n",
      "300/300 [==============================] - 167s 543ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 1.00000, saving model to C:\\NLP\\SentimentAnalysis\\saved_models\\model_5.h5\n",
      "Epoch 2/3\n",
      "300/300 [==============================] - 161s 537ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 1.00000\n",
      "Epoch 3/3\n",
      "300/300 [==============================] - 161s 537ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "#import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "logdir = str.format('logs\\scalars\\{0}', datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('princeton-nlp/sup-simcse-bert-base-uncased')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-bert-base-uncased')\n",
    "#tokenizer.encode(add_special_tokens=True, text=)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True) \n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDAITON_LOSS = []\n",
    "batch_size = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "save_dir = os.getcwd()+'\\saved_models'\n",
    "fold_var = 1\n",
    "Y = df[['Sentiment']]\n",
    "#X = df['Sentence']\n",
    "X = df['Sentence'] +\" : \"+ df['Entity']\n",
    "for train_index, val_index in skf.split(X,Y):\n",
    "    training_data = X.iloc[train_index]\n",
    "    validation_data = X.iloc[val_index]\n",
    "    #print(train_index + '| '+ training_data + ' | |'+ val_index + '|' +validation_data)\n",
    "    #print(str.format('{0} | {1} || {2} | {3}', train_index, training_data, val_index, validation_data))\n",
    "    \n",
    "    #-------------------------------------------------Train DataSet ----------------------------------------------------#\n",
    "    num_samples = len(training_data)\n",
    "    ###arr = validation_data.values\n",
    "    \n",
    "    train_lbl =  Y.iloc[train_index]\n",
    "    train_arr = np.array(train_lbl['Sentiment'].values.tolist())\n",
    "    \n",
    "    print(train_arr)\n",
    "\n",
    "    train_labels = train_arr #np.zeros((num_samples, train_arr.max()+1))\n",
    "    print(train_labels)\n",
    "    #print(train_labels[np.arange(num_samples), train_arr] = 1)\n",
    "    #train_labels[np.arange(num_samples), train_arr] = 1\n",
    "\n",
    "\n",
    "    # tokenize - this time returning Numpy tensors\n",
    "    \n",
    "    train_tokens = tokenizer(training_data.tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np') \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_tokens['input_ids'], \n",
    "                                                        train_tokens['attention_mask'], \n",
    "                                                        train_labels))\n",
    "    train_dataset = train_dataset.map(map_func)\n",
    "    #dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "  \n",
    "\n",
    "    print(train_dataset.take(1))\n",
    "    #-------------------------------------------------Validation DataSet ----------------------------------------------------#\n",
    "    num_samples = len(validation_data)\n",
    "    ###arr = validation_data.values\n",
    "    \n",
    "    val_lbl =  Y.iloc[val_index]\n",
    "    val_arr = np.array(val_lbl['Sentiment'].values.tolist())\n",
    "    ###arr = lbl['Sentiment']\n",
    "    #print(type(arr))\n",
    "    \n",
    "    #print(num_samples)\n",
    "    #print(arr)\n",
    "    val_labels = val_arr#np.zeros((num_samples, val_arr.max()+1))\n",
    "    #print(labels.shape)\n",
    "    \n",
    "   # val_labels[np.arange(num_samples), val_arr] = 1\n",
    "\n",
    "    print(val_labels)\n",
    "\n",
    "    # tokenize - this time returning Numpy tensors\n",
    "    val_tokens = tokenizer(validation_data.tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np')\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((val_tokens['input_ids'], \n",
    "                                                             val_tokens['attention_mask'], \n",
    "                                                             val_labels))\n",
    "    validation_dataset = validation_dataset.map(map_func)\n",
    "    validation_dataset = validation_dataset.batch(batch_size, drop_remainder=True)\n",
    "    #batch_size = 16\n",
    "\n",
    "    print(validation_dataset.take(1))\n",
    "\n",
    "    model = create_Model()\n",
    "\n",
    "    \n",
    "    # CREATE CALLBACKS\n",
    "    '''\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var), \n",
    "                            monitor='val_accuracy', verbose=1, \n",
    "                            save_best_only=True, mode='max')\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    \n",
    "    callbacks_list = [checkpoint, tensorboard_callback]\n",
    "    \n",
    "    history = model.fit(train_dataset, validation_data=validation_dataset, epochs=3, callbacks=callbacks_list, verbose=1)\n",
    "    \n",
    "    model.load_weights(save_dir +\"\\model_\"+str(fold_var)+\".h5\")\n",
    "    '''\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    checkpoint_filepath = save_dir + get_model_name(fold_var)\n",
    "    if(fold_var>1):\n",
    "        save_dir + get_model_name(fold_var)\n",
    "        model.load_weights(save_dir + get_model_name(fold_var-1))\n",
    "        \n",
    "    print(get_model_name(fold_var))\n",
    "    print(checkpoint_filepath)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                                   save_weights_only=False,\n",
    "                                                                   monitor='val_accuracy',\n",
    "                                                                   mode='max',\n",
    "                                                                   verbose=1,\n",
    "                                                                   save_best_only=True)\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='precision', patience=5)\n",
    "\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen\n",
    "    # so far.\n",
    "    model.fit(train_dataset, \n",
    "              validation_data=validation_dataset,\n",
    "              epochs=EPOCHS, \n",
    "              callbacks=[model_checkpoint_callback, tensorboard_callback, earlyStopping], \n",
    "              verbose=1)\n",
    "\n",
    "    # The model weights (that are considered the best) are loaded into the model.\n",
    "    model.load_weights(save_dir +\"\\model_\"+str(fold_var)+\".h5\")\n",
    "    #model.load_weights(checkpoint_filepath +\"\\model_\"+str(fold_var)+\".h5\")\n",
    "    #----------------------------------------------------------------------------\n",
    "    #results = model.evaluate(validation_data)\n",
    "    #results = dict(zip(model.metrics_names,results))\n",
    "    \n",
    "    #VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    #VALIDATION_LOSS.append(results['loss'])\n",
    "    \n",
    "    fold_var +=1\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d328a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
